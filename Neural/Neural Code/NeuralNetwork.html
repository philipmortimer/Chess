<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<title>NeuralNetwork.java</title>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<style type="text/css">
<!--
body {color: #000000; background-color: #ffffff; font-family: Monospaced}
pre {color: #000000; background-color: #ffffff; font-family: Monospaced}
table {color: #000000; background-color: #e9e8e2; font-family: Monospaced}
.string {color: #ce7b00}
.literal {color: #0000e6}
.comment {color: #969696}
.ST0 {color: #969696; font-family: Monospaced; font-weight: bold}
-->
</style>
</head>
<body>
<table width="100%"><tr><td align="center">C:\Users\mortimer\OneDrive - gowercollegeswansea.ac.uk\Documents\College\YEAR 12\COMPUTER SCIENCE\Books attempt\DataBase\PhilipM CS5 Software Development Neural Network train\src\philipm\cs5\software\development\neural\network\train\NeuralNetwork.java</td></tr></table>
<pre>
  1 <span class="comment">/*</span>
  2 <span class="comment"> * To change this license header, choose License Headers in Project Properties.</span>
  3 <span class="comment"> * To change this template file, choose Tools | Templates</span>
  4 <span class="comment"> * and open the template in the editor.</span>
  5 <span class="comment"> */</span>
  6 <span class="literal">package</span> philipm.cs5.software.development.neural.network.train;
  7 
  8 <span class="literal">import</span> java.io.BufferedReader;
  9 <span class="literal">import</span> java.io.BufferedWriter;
 10 <span class="literal">import</span> java.io.FileReader;
 11 <span class="literal">import</span> java.io.FileWriter;
 12 <span class="literal">import</span> java.io.IOException;
 13 <span class="literal">import</span> java.util.Random;
 14 
 15 <span class="comment">/**</span>
 16 <span class="comment"> * </span><span class="ST0">This</span> <span class="ST0">class</span> <span class="ST0">is</span> <span class="ST0">used</span> <span class="ST0">to</span> <span class="ST0">train</span> <span class="ST0">my</span> <span class="ST0">chess</span> <span class="ST0">neural</span> <span class="ST0">network</span>
 17 <span class="comment"> * </span><span class="ST0">@author</span> <span class="comment">mortimer</span>
 18  <span class="comment">*/</span>
 19 <span class="literal">public</span> <span class="literal">class</span> NeuralNetwork {
 20     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span> NET_STRUCTURE_LENGTH;<span class="comment">//stores the number of weights and biases in the neural network</span>
 21     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span> NET_STRUCTURE_LENGTH_MINUS_ONE;
 22     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span> ALL_LENGTH;<span class="comment">//stores the length of all weights and biases array</span>
 23     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span>[]TOT_NEURON;
 24     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span>[]CURRENT_ALL;
 25     <span class="literal">private</span> <span class="literal">final</span> Random RAND = <span class="literal">new</span> Random();<span class="comment">//creates a random object to be used in network</span>
 26     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span>[]INDEX_FOR_ALL_WEIGHTS_ERROR;<span class="comment">//stores the indexes to be used in backpropogation to optimise the process</span>
 27     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span>[]INDEX_FOR_ACTIVS_ERROR;<span class="comment">//stores the indexes to be used when calculating the error of the network</span>
 28     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span> ACTIV_IN;
 29     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span>[]NET_STRUCTURE;<span class="comment">//stores the network structure</span>
 30     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">double</span>[]ALL_WEIGHTS_AND_BIASES;<span class="comment">// stores all weights and biases in the network</span>
 31     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span> TOTAL_NEURONS;
 32     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span> TOTAL_NEURONS_MINUS_LAST_LAYER;<span class="comment">//stores the number of nodes in the network that are not in the output layer</span>
 33     <span class="literal">private</span> <span class="literal">double</span>[] nodeValuePreActivation;<span class="comment">//stores the input to nodes in a given layer before the activation function is applied</span>
 34     <span class="literal">private</span> <span class="literal">double</span> []nodeValuePostActivation;<span class="comment">//stores the post activation values of a nodes in a layer in the network</span>
 35     <span class="literal">private</span> <span class="literal">static</span> <span class="literal">final</span> String FILE_NAME = <span class="string">&quot;</span><span class="string">networkInfo.txt</span><span class="string">&quot;</span>;<span class="comment">//stores file name where network data is stored</span>
 36     <span class="literal">private</span> <span class="literal">double</span> timeStep; <span class="comment">//stores the time step</span>
 37     <span class="literal">private</span> <span class="literal">double</span> previousMHat[];<span class="comment">//stores previous values of MHat - used to allow training using ADAM optimiser over multiple training method calls</span>
 38     <span class="literal">private</span> <span class="literal">double</span> previousVHat[];<span class="comment">//stores previous values of VHat - used to allow training using ADAM optimiser over multiple training method calls</span>
 39     <span class="literal">private</span> <span class="literal">static</span> <span class="literal">final</span> String BEST_NET_FILE_NAME=<span class="string">&quot;</span><span class="string">bestNet.txt</span><span class="string">&quot;</span>;<span class="comment">//stores the name of the text file that is used to store the best neural network</span>
 40     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">boolean</span> []NEURONS_DROPPED;<span class="comment">//stores which neurons are dropped</span>
 41     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span> LENGTH_OF_NEURONS_DROPPED_ARRAY;
 42     <span class="literal">private</span> <span class="literal">final</span> <span class="literal">int</span> INDEX_CACHE[];
 43     <span class="comment">/**</span>
 44 <span class="comment">     * </span><span class="ST0">Creates</span> <span class="ST0">a</span> <span class="ST0">neural</span> <span class="ST0">network</span> <span class="ST0">of</span> <span class="ST0">the</span> <span class="ST0">desired</span> <span class="ST0">architecture</span> <span class="ST0">using</span> <span class="ST0">reLu</span> <span class="ST0">in</span> <span class="ST0">all</span> <span class="ST0">layer</span> <span class="ST0">bar</span> <span class="ST0">the</span> <span class="ST0">output</span> <span class="ST0">layer</span><span class="ST0">, </span><span class="ST0">where</span> <span class="ST0">the</span> <span class="ST0">softmax</span> <span class="ST0">function</span> <span class="ST0">is</span> <span class="ST0">used</span>
 45 <span class="comment">     * </span><span class="ST0">@param</span> <span class="comment">netStructure</span> <span class="comment">The</span> <span class="comment">network</span> <span class="comment">structure</span><span class="comment">, </span><span class="comment">with</span> <span class="comment">element</span><span class="comment"> 0 </span><span class="comment">specifying</span> <span class="comment">the</span> <span class="comment">number</span> <span class="comment">of</span> <span class="comment">nodes</span> <span class="comment">in</span> <span class="comment">the</span> <span class="comment">input</span> <span class="comment">layer</span> <span class="comment">and</span> <span class="comment">the</span> <span class="comment">final</span> <span class="comment">element</span> <span class="comment">specifying</span> <span class="comment">the</span> <span class="comment">number</span> <span class="comment">of</span> <span class="comment">nodes</span> <span class="comment">in</span> <span class="comment">the</span> <span class="comment">output</span> <span class="comment">layer</span>
 46      <span class="comment">*/</span>
 47     <span class="literal">public</span>  NeuralNetwork(<span class="literal">int</span>[]netStructure){
 48         <span class="comment">//initilaises various constants used to speed up the backpropogation process</span>
 49         <span class="literal">this</span>.NET_STRUCTURE = netStructure;
 50         <span class="literal">this</span>.NET_STRUCTURE_LENGTH=<span class="literal">this</span>.NET_STRUCTURE.length;
 51         <span class="literal">this</span>.NET_STRUCTURE_LENGTH_MINUS_ONE=<span class="literal">this</span>.NET_STRUCTURE_LENGTH-1;
 52         <span class="literal">int</span> totalWeightsAndBiases=0;
 53         <span class="literal">int</span> totalNeurons=0;
 54         INDEX_CACHE=<span class="literal">new</span> <span class="literal">int</span>[netStructure.length-1];
 55         <span class="comment">//calculates the total number of neurons as well as the total number of weights and biases in the network</span>
 56         <span class="literal">for</span>(<span class="literal">int</span> i =0;i&lt;netStructure.length;i++){
 57             <span class="literal">if</span>(i!=netStructure.length-1 &amp;&amp; i!=0){
 58                 INDEX_CACHE[i]=totalNeurons-netStructure[0];
 59             }
 60             totalNeurons=totalNeurons+<span class="literal">this</span>.NET_STRUCTURE[i];
 61             <span class="literal">if</span>(i!=0){
 62                 totalWeightsAndBiases = totalWeightsAndBiases+<span class="literal">this</span>.NET_STRUCTURE[i]+(<span class="literal">this</span>.NET_STRUCTURE[i]*<span class="literal">this</span>.NET_STRUCTURE[i-1]);
 63             }
 64         }
 65         NEURONS_DROPPED=<span class="literal">new</span> <span class="literal">boolean</span>[totalNeurons-netStructure[0]-netStructure[netStructure.length-1]];
 66         LENGTH_OF_NEURONS_DROPPED_ARRAY=NEURONS_DROPPED.length;
 67         <span class="comment">//initilaises more constants used for optimisation purposes</span>
 68         <span class="literal">this</span>.TOTAL_NEURONS = totalNeurons;
 69         <span class="literal">this</span>.TOTAL_NEURONS_MINUS_LAST_LAYER=<span class="literal">this</span>.TOTAL_NEURONS-<span class="literal">this</span>.NET_STRUCTURE[<span class="literal">this</span>.NET_STRUCTURE_LENGTH_MINUS_ONE];
 70         <span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES=<span class="literal">new</span> <span class="literal">double</span>[totalWeightsAndBiases];
 71         <span class="literal">this</span>.ALL_LENGTH=totalWeightsAndBiases;
 72         <span class="literal">this</span>.ACTIV_IN=<span class="literal">this</span>.TOTAL_NEURONS-<span class="literal">this</span>.NET_STRUCTURE[<span class="literal">this</span>.NET_STRUCTURE.length-1];
 73         <span class="literal">int</span> currentIndex=0;
 74         <span class="comment">//initialises the value of weights and biases in the network to values that make learning optimal</span>
 75         <span class="literal">for</span>(<span class="literal">int</span> layerIndex=0;layerIndex&lt;<span class="literal">this</span>.NET_STRUCTURE.length;layerIndex++){
 76             <span class="literal">if</span>(layerIndex!=0){
 77                 <span class="literal">for</span>(<span class="literal">int</span> currentLayerIndex=0;currentLayerIndex&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex];currentLayerIndex++){
 78                     <span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES[currentIndex]=0.0;<span class="comment">//all biases are initiliased to 0</span>
 79                     currentIndex++;
 80                     <span class="literal">for</span>(<span class="literal">int</span> prevLayerIndex=0;prevLayerIndex&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex-1];prevLayerIndex++){<span class="comment">//weigths leading into bias</span>
 81                         <span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES[currentIndex]=getRandDoubleBetweenOneAndMinusOne(<span class="literal">this</span>.NET_STRUCTURE[layerIndex-1],<span class="literal">this</span>.NET_STRUCTURE[layerIndex]);<span class="comment">//use xavier initialisation to intialise the network</span>
 82                         currentIndex++;
 83                     }
 84                 }
 85             }
 86         }
 87         <span class="comment">//the backpropogation process is perfromed on made up data to allow for fast backpropogation during the training stage of the network</span>
 88         <span class="literal">this</span>.feedThroughNet(<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.NET_STRUCTURE[0]]);<span class="comment">//feeds a random input into the network</span>
 89         <span class="literal">double</span>[]desiredOutput=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.NET_STRUCTURE[<span class="literal">this</span>.NET_STRUCTURE_LENGTH_MINUS_ONE]];<span class="comment">//creates a random desired output</span>
 90         <span class="literal">double</span>[]errorInLayer= <span class="literal">new</span> <span class="literal">double</span>[1];<span class="comment">//stores the network error</span>
 91         <span class="literal">double</span>[]errorInLayerAbove=<span class="literal">new</span> <span class="literal">double</span>[1];<span class="comment">//stores error in layer above</span>
 92         <span class="literal">double</span> sumOfErrorAndWeights;
 93         <span class="literal">double</span> grad[]=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES.length];
 94         <span class="literal">int</span> indexForAllWhenDoingWeightsAndError;
 95         <span class="literal">int</span> indexWeightErrorChange;
 96         <span class="literal">int</span> indexForActivsError;
 97         <span class="literal">int</span> totalAllExplored;
 98         <span class="literal">int</span> indexForAllGrad;
 99         <span class="literal">int</span> totNeuronsExplored;
100         <span class="literal">int</span> indexForActivsGrad;
101         <span class="literal">int</span> activIndex;
102         <span class="comment">//this process gets the indexs accessed during training to avoid repeating the same calculation over and over again</span>
103         <span class="literal">int</span>[] arrayIndexForAllWhenDoingWeightsAndError=<span class="literal">new</span> <span class="literal">int</span>[<span class="literal">this</span>.NET_STRUCTURE.length];<span class="comment">//these arrays are used to speed of the backpropogation process</span>
104         <span class="literal">int</span>[]arrayForIndexForActivsError= <span class="literal">new</span> <span class="literal">int</span>[<span class="literal">this</span>.NET_STRUCTURE.length];
105         <span class="literal">int</span>[]arrayForIndexForAllGrad=<span class="literal">new</span> <span class="literal">int</span>[<span class="literal">this</span>.NET_STRUCTURE.length];
106         <span class="literal">int</span> []arrayForTotNeuronsExplored=<span class="literal">new</span> <span class="literal">int</span>[<span class="literal">this</span>.NET_STRUCTURE.length];
107         <span class="literal">for</span>(<span class="literal">int</span> layerIndex=<span class="literal">this</span>.NET_STRUCTURE.length-1;layerIndex&gt;0;layerIndex--){
108             errorInLayer=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.NET_STRUCTURE[layerIndex]];
109             <span class="literal">if</span>(layerIndex==<span class="literal">this</span>.NET_STRUCTURE.length-1){
110                 activIndex=<span class="literal">this</span>.TOTAL_NEURONS-<span class="literal">this</span>.NET_STRUCTURE[<span class="literal">this</span>.NET_STRUCTURE.length-1];
111                 <span class="literal">for</span>(<span class="literal">int</span> neuronInLayer=0;neuronInLayer&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex];neuronInLayer++){
112                     errorInLayer[neuronInLayer]=(<span class="literal">this</span>.nodeValuePostActivation[activIndex]-desiredOutput[neuronInLayer])*activationDerivative(<span class="literal">this</span>.nodeValuePreActivation[activIndex]);
113                     activIndex++;
114                 }
115             }<span class="literal">else</span>{
116                 indexForAllWhenDoingWeightsAndError=0;
117                 <span class="literal">for</span>(<span class="literal">int</span> layer=1;layer&lt;layerIndex+1;layer++){<span class="comment">//calcs first index in all of next layer</span>
118                     indexForAllWhenDoingWeightsAndError=indexForAllWhenDoingWeightsAndError+(<span class="literal">this</span>.NET_STRUCTURE[layer]*<span class="literal">this</span>.NET_STRUCTURE[layer-1]+<span class="literal">this</span>.NET_STRUCTURE[layer]);
119                 }
120                 arrayIndexForAllWhenDoingWeightsAndError[layerIndex]=indexForAllWhenDoingWeightsAndError;
121                 indexForActivsError=0;
122                 <span class="literal">for</span>(<span class="literal">int</span> layer=0;layer&lt;layerIndex;layer++){<span class="comment">//frist index of activ in current layer</span>
123                     indexForActivsError=indexForActivsError+<span class="literal">this</span>.NET_STRUCTURE[layer];
124                 }
125                 arrayForIndexForActivsError[layerIndex]=indexForActivsError;
126                 <span class="literal">for</span>(<span class="literal">int</span> neuronInLayer = 0;neuronInLayer&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex];neuronInLayer++){
127                     indexWeightErrorChange=indexForAllWhenDoingWeightsAndError+1+neuronInLayer;<span class="comment">//first index of weight</span>
128                     sumOfErrorAndWeights=0;
129                     <span class="literal">for</span>(<span class="literal">int</span> neuronInLayerAfter=0;neuronInLayerAfter&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex+1];neuronInLayerAfter++){
130                         sumOfErrorAndWeights=sumOfErrorAndWeights+(errorInLayerAbove[neuronInLayerAfter]*<span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES[indexWeightErrorChange]);
131                         indexWeightErrorChange=indexWeightErrorChange+1+<span class="literal">this</span>.NET_STRUCTURE[layerIndex];
132                     }
133                     errorInLayer[neuronInLayer]=sumOfErrorAndWeights*activationDerivative(<span class="literal">this</span>.nodeValuePreActivation[indexForActivsError]);
134                     indexForActivsError++;
135                 }
136             }
137             <span class="comment">//errorInLayer has been generated</span>
138             totalAllExplored=0;
139             <span class="literal">for</span>(<span class="literal">int</span> layer = <span class="literal">this</span>.NET_STRUCTURE.length-1;layer&gt;layerIndex-1;layer=layer-1){
140                 totalAllExplored=totalAllExplored + (<span class="literal">this</span>.NET_STRUCTURE[layer]*<span class="literal">this</span>.NET_STRUCTURE[layer-1]+<span class="literal">this</span>.NET_STRUCTURE[layer]);
141             }
142             indexForAllGrad=<span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES.length-totalAllExplored;<span class="comment">//first all of biases in this layer</span>
143             arrayForIndexForAllGrad[layerIndex]=indexForAllGrad;
144             totNeuronsExplored=0;
145             <span class="literal">for</span>(<span class="literal">int</span> layer=0;layer&lt;layerIndex-1;layer++){<span class="comment">//first index of activs in layer before</span>
146                 totNeuronsExplored=totNeuronsExplored+<span class="literal">this</span>.NET_STRUCTURE[layer];
147             }        
148             arrayForTotNeuronsExplored[layerIndex]=totNeuronsExplored;
149             <span class="literal">for</span>(<span class="literal">int</span> neuronInLayer=0;neuronInLayer&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex];neuronInLayer++){
150                 grad[indexForAllGrad]=errorInLayer[neuronInLayer];<span class="comment">//sets grad for bias;</span>
151                 indexForAllGrad++;
152                 indexForActivsGrad=totNeuronsExplored;
153                 <span class="literal">for</span>(<span class="literal">int</span> neuronInLayerBefore=0;neuronInLayerBefore&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex-1];neuronInLayerBefore++){
154                     grad[indexForAllGrad]=errorInLayer[neuronInLayer]*(<span class="literal">this</span>.nodeValuePostActivation[indexForActivsGrad]);
155                     indexForActivsGrad++;
156                     indexForAllGrad++;
157                 }
158             }
159             errorInLayerAbove=<span class="literal">new</span> <span class="literal">double</span>[errorInLayer.length];
160             <span class="literal">for</span>(<span class="literal">int</span> i =0;i&lt;errorInLayer.length;i++){
161                 errorInLayerAbove[i]=errorInLayer[i];
162             }
163         }
164         <span class="comment">//the backpropogation mock trial run is finished and now the indexes accessed are stored to avoid needless calculations when backpropogation is carried out during the training phase</span>
165         <span class="literal">this</span>.INDEX_FOR_ALL_WEIGHTS_ERROR=arrayIndexForAllWhenDoingWeightsAndError;
166         <span class="literal">this</span>.INDEX_FOR_ACTIVS_ERROR=arrayForIndexForActivsError;
167         <span class="literal">this</span>.CURRENT_ALL=arrayForIndexForAllGrad;
168         <span class="literal">this</span>.TOT_NEURON=arrayForTotNeuronsExplored;
169     }
170     <span class="comment">/**</span>
171 <span class="comment">     * </span><span class="ST0">Loads</span> <span class="ST0">the</span> <span class="ST0">best</span> <span class="ST0">neural</span> <span class="ST0">network</span> <span class="ST0">with</span> <span class="ST0">the</span> <span class="ST0">architecture</span> <span class="ST0">of</span> <span class="ST0">the</span> <span class="ST0">network</span> <span class="ST0">that</span> <span class="ST0">is</span> <span class="ST0">stored</span> <span class="ST0">in</span> <span class="ST0">the</span> <span class="ST0">network</span> <span class="ST0">info</span> <span class="ST0">text</span> <span class="ST0">file</span>
172 <span class="comment">     * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">weights</span> <span class="comment">and</span> <span class="comment">biases</span> <span class="comment">for</span> <span class="comment">the</span> <span class="comment">best</span> <span class="comment">network</span><span class="comment">.</span>
173      <span class="comment">*/</span>
174     <span class="literal">public</span> <span class="literal">double</span>[]loadBestNetWithThisArchitecture(){
175         <span class="literal">double</span> []bestNet=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES.length];
176         <span class="literal">double</span> bestAcc=-0.5;
177         <span class="literal">int</span> lineNoEnd;
178         <span class="literal">int</span> lineNoStart=0;
179         <span class="literal">double</span> currentAcc=0;
180         <span class="literal">boolean</span> isBest=<span class="literal">true</span>;
181         <span class="literal">boolean</span> isSameArch=<span class="literal">false</span>;
182         <span class="literal">try</span>{
183             <span class="comment">//loads text file into memory</span>
184             FileReader read = <span class="literal">new</span> FileReader(<span class="literal">this</span>.FILE_NAME);
185             BufferedReader buff = <span class="literal">new</span> BufferedReader(read);
186             Object[]a=buff.lines().toArray();
187             String []s = <span class="literal">new</span> String[a.length];
188             <span class="literal">for</span>(<span class="literal">int</span> i=0;i&lt;a.length;i++){
189                 s[i]=String.valueOf(a[i]);
190             }
191             <span class="comment">//loops through evert line of the text file</span>
192             <span class="literal">for</span>(<span class="literal">int</span> i=0;i&lt;s.length;i++){
193                 <span class="literal">if</span>(s[i].contains(<span class="string">&quot;</span><span class="string">Network Structure</span><span class="string">&quot;</span>)){<span class="comment">//checks to see if line is descirbing neural network structure </span>
194                     isSameArch=<span class="literal">false</span>;
195                     String str=<span class="string">&quot;</span><span class="string">Network Structure </span><span class="string">&quot;</span>;
196                     <span class="literal">for</span>(<span class="literal">int</span> index=0;index&lt;<span class="literal">this</span>.NET_STRUCTURE.length;index++){
197                         str=str+String.valueOf(<span class="literal">this</span>.NET_STRUCTURE[index])+<span class="string">&quot;</span>  <span class="string">&quot;</span>;
198                     }
199                     <span class="literal">if</span>(s[i].equals(str)){<span class="comment">//checks to see whether the architecture is the same as the current network</span>
200                         isSameArch=<span class="literal">true</span>;
201                     }
202                 }
203                 <span class="literal">if</span>(s[i].contains(<span class="string">&quot;</span><span class="string">Best network (with accuracy</span><span class="string">&quot;</span>)&amp;&amp;isSameArch==<span class="literal">true</span>){<span class="comment">//checks to see if new network is appropriate</span>
204                     isBest=<span class="literal">false</span>;
205                     String num=<span class="string">&quot;&quot;</span>;
206                     <span class="comment">//gets accuracy of best network</span>
207                     <span class="literal">for</span>(<span class="literal">int</span> c=0;c&lt;s[i].length();c++){
208                         <span class="literal">boolean</span> isNumOrDot=<span class="literal">true</span>;
209                         <span class="literal">try</span>{
210                             <span class="literal">double</span> f=Double.parseDouble(String.valueOf((s[i].charAt(c))));
211                         }<span class="literal">catch</span>(NumberFormatException e){
212                             <span class="literal">if</span>(s[i].charAt(c)!=<span class="string">&#39;</span><span class="string">.</span><span class="string">&#39;</span>){
213                                 isNumOrDot=<span class="literal">false</span>;
214                             }
215                         }
216                         <span class="literal">if</span>(isNumOrDot==<span class="literal">true</span>){
217                             num =num+s[i].charAt(c);
218                         }
219                     }
220                     currentAcc=Double.parseDouble(num);
221                     <span class="literal">if</span>(currentAcc&gt;bestAcc&amp;&amp;isSameArch==<span class="literal">true</span>){<span class="comment">//updates best accuracy if appropriate</span>
222                         bestAcc=currentAcc;
223                         isBest=<span class="literal">true</span>;
224                     }
225                     <span class="literal">if</span>(isBest==<span class="literal">true</span>&amp;&amp;isSameArch==<span class="literal">true</span>){<span class="comment">//stores network as best network if appropraite</span>
226                         lineNoStart=i+1;
227                         lineNoEnd=i+bestNet.length;
228                         <span class="literal">int</span> bestIn=0;
229                         <span class="literal">for</span>(<span class="literal">int</span> element=lineNoStart;element&lt;=lineNoEnd;element++){
230                             bestNet[bestIn]=Double.parseDouble(s[element]);
231                             bestIn++;
232                         }
233                     }
234                 }
235             }
236         }<span class="literal">catch</span>(IOException e){
237             System.out.println(<span class="string">&quot;</span><span class="string">errro </span><span class="string">&quot;</span>+e+<span class="string">&quot;</span><span class="string"> with file </span><span class="string">&quot;</span>+FILE_NAME);
238             e.printStackTrace();
239         }
240         <span class="literal">return</span> bestNet;
241     }
242     <span class="comment">/**</span>
243 <span class="comment">     * </span><span class="ST0">Loads</span> <span class="ST0">the</span> <span class="ST0">array</span> <span class="ST0">stored</span> <span class="ST0">in</span> <span class="ST0">the</span> <span class="ST0">best</span> <span class="ST0">net</span> <span class="ST0">text</span> <span class="ST0">file</span><span class="ST0">, </span><span class="ST0">assuming</span> <span class="ST0">it</span> <span class="ST0">is</span> <span class="ST0">intended</span> <span class="ST0">for</span> <span class="ST0">a</span> <span class="ST0">network</span> <span class="ST0">with</span> <span class="ST0">the</span> <span class="ST0">architecture</span> <span class="ST0">of</span> <span class="ST0">the</span> <span class="ST0">current</span> <span class="ST0">network</span><span class="ST0">.</span>
244 <span class="comment">     * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">weights</span> <span class="comment">and</span> <span class="comment">biases</span> <span class="comment">array</span>
245 <span class="comment">     * </span><span class="ST0">@throws</span> <span class="comment">Exception</span> <span class="comment">An</span> <span class="comment">exception</span> <span class="comment">may</span> <span class="comment">occur</span> <span class="comment">if</span> <span class="comment">the</span> <span class="comment">best</span> <span class="comment">net</span> <span class="comment">file</span> <span class="comment">does</span> <span class="comment">not</span> <span class="comment">match</span> <span class="comment">the</span> <span class="comment">network</span> <span class="comment">structure</span><span class="comment">.</span>
246      <span class="comment">*/</span>
247     <span class="literal">public</span> <span class="literal">double</span>[]loadBestNetFile() <span class="literal">throws</span> Exception{
248         <span class="literal">double</span>[]best = <span class="literal">new</span> <span class="literal">double</span>[ALL_WEIGHTS_AND_BIASES.length];
249         FileReader read = <span class="literal">new</span> FileReader(BEST_NET_FILE_NAME);
250         BufferedReader buffRead = <span class="literal">new</span> BufferedReader(read);
251         <span class="literal">int</span> index=0;
252         String lineRead;
253         <span class="literal">while</span>((lineRead=buffRead.readLine())!=<span class="literal">null</span>){
254             best[index]=Double.parseDouble(lineRead);index++;
255         }
256         buffRead.close();read.close();
257         <span class="literal">return</span> best;
258     }
259     <span class="comment">/**</span>
260 <span class="comment">     * </span><span class="ST0">This</span> <span class="ST0">is</span> <span class="ST0">the</span> <span class="ST0">hyperbolic</span> <span class="ST0">tangent</span> <span class="ST0">function</span><span class="ST0">.</span>
261 <span class="comment">     * </span><span class="ST0">@param</span> <span class="comment">x</span> <span class="comment">The</span> <span class="comment">input</span> 
262 <span class="comment">     * </span><span class="ST0">@return</span> <span class="comment">the</span> <span class="comment">output</span> <span class="comment">of</span> <span class="comment">tanH</span>
263      <span class="comment">*/</span>
264      <span class="literal">private</span> <span class="literal">static</span> <span class="literal">double</span> tanH(<span class="literal">double</span> x){
265          <span class="literal">return</span> ((Math.pow(Math.E, x)-Math.pow(Math.E,-x))/(Math.pow(Math.E,x)+Math.pow(Math.E,-x)));
266      }
267      <span class="comment">/**</span>
268 <span class="comment">      * </span><span class="ST0">This</span> <span class="ST0">calculates</span> <span class="ST0">the</span> <span class="ST0">the</span> <span class="ST0">first</span> <span class="ST0">order</span> <span class="ST0">derivative</span> <span class="ST0">of</span> <span class="ST0">tanH</span>
269 <span class="comment">      * </span><span class="ST0">@param</span> <span class="comment">x</span> <span class="comment">The</span> <span class="comment">input</span>
270 <span class="comment">      * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">output</span>
271       <span class="comment">*/</span>
272      <span class="literal">private</span> <span class="literal">static</span> <span class="literal">double</span> gradTanH(<span class="literal">double</span> x){
273          <span class="literal">return</span>(1.0-(Math.pow(tanH(x),2)));
274      }
275      <span class="comment">/**</span>
276 <span class="comment">      * </span><span class="ST0">This</span> <span class="ST0">function</span> <span class="ST0">executes</span> <span class="ST0">the</span> <span class="ST0">reLu</span> <span class="ST0">function</span>
277 <span class="comment">      * </span><span class="ST0">@param</span> <span class="comment">x</span> <span class="comment">The</span> <span class="comment">input</span>
278 <span class="comment">      * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">output</span>
279       <span class="comment">*/</span>
280      <span class="literal">private</span> <span class="literal">static</span> <span class="literal">double</span> reLU(<span class="literal">double</span> x){
281          <span class="literal">if</span>(x&gt;0.0){
282              <span class="literal">return</span> x;
283          }<span class="literal">else</span>{
284              <span class="literal">return</span> 0.0;
285          }
286      }
287      <span class="comment">/**</span>
288 <span class="comment">      * </span><span class="ST0">This</span> <span class="ST0">calculates</span> <span class="ST0">the</span> <span class="ST0">first</span> <span class="ST0">oder</span> <span class="ST0">derivative</span> <span class="ST0">of</span> <span class="ST0">reLu</span><span class="ST0">.</span> <span class="comment">Note</span><span class="comment"> ((</span><span class="comment">dy</span><span class="comment">/</span><span class="comment">dx</span><span class="comment">)</span><span class="comment">reLU</span><span class="comment">(0)) </span><span class="comment">is</span> <span class="comment">handled</span> <span class="comment">as</span><span class="comment"> 0 (</span><span class="comment">although</span> <span class="comment">reLU</span> <span class="comment">is</span> <span class="comment">not</span> <span class="comment">differentiable</span> <span class="comment">at</span> <span class="comment">x</span><span class="comment"> = 0)</span>
289 <span class="comment">      * </span><span class="ST0">@param</span> <span class="comment">x</span> <span class="comment">The</span> <span class="comment">input</span>
290 <span class="comment">      * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">output</span>
291       <span class="comment">*/</span>
292      <span class="literal">private</span> <span class="literal">static</span> <span class="literal">double</span> gradRELU(<span class="literal">double</span> x){
293          <span class="literal">if</span>(x&gt;0.0){
294              <span class="literal">return</span> 1.0;
295          }<span class="literal">else</span>{
296              <span class="literal">return</span> 0.0;
297         }
298      }
299      <span class="comment">/**</span>
300 <span class="comment">      * </span><span class="ST0">Performs</span> <span class="ST0">the</span> <span class="ST0">softmax</span> <span class="ST0">function</span> <span class="ST0">on</span> <span class="ST0">an</span> <span class="ST0">input</span> <span class="ST0">vector</span>
301 <span class="comment">      * </span><span class="ST0">@param</span> <span class="comment">input</span> <span class="comment">The</span> <span class="comment">input</span> <span class="comment">vector</span>
302 <span class="comment">      * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">output</span> <span class="comment">vector</span>
303       <span class="comment">*/</span>
304      <span class="literal">private</span> <span class="literal">static</span> <span class="literal">double</span>[]softmax(<span class="literal">double</span>[]input){
305          <span class="literal">double</span> sum=0.0;<span class="comment">//calculates the sum of the e^x for each element in the array</span>
306          <span class="literal">double</span>[]ret=<span class="literal">new</span> <span class="literal">double</span>[input.length];<span class="comment">//stores the vector to return</span>
307          <span class="literal">double</span>[]exp=<span class="literal">new</span> <span class="literal">double</span>[input.length];
308          <span class="literal">for</span>(<span class="literal">int</span> i =0;i&lt;input.length;i++){
309              exp[i]=Math.exp(input[i]);
310              sum =sum+exp[i];
311          }
312          <span class="literal">for</span>(<span class="literal">int</span> i =0;i&lt;input.length;i++){
313              ret[i]=exp[i]/sum;
314          }
315          <span class="literal">return</span> ret;
316      }
317      <span class="comment">/**</span>
318 <span class="comment">      * </span><span class="ST0">This</span> <span class="ST0">method</span> <span class="ST0">calculates</span> <span class="ST0">the</span> <span class="ST0">gradient</span> <span class="ST0">of</span> <span class="ST0">each</span> <span class="ST0">parameter</span> <span class="ST0">with</span> <span class="ST0">respect</span> <span class="ST0">to</span> <span class="ST0">the</span> <span class="ST0">loss</span> <span class="ST0">function</span><span class="ST0">.</span> <span class="comment">This</span> <span class="comment">is</span> <span class="comment">an</span> <span class="comment">optimised</span> <span class="comment">implementation</span> <span class="comment">of</span> <span class="comment">the</span> <span class="comment">backpropagation</span> <span class="comment">algorithm</span><span class="comment">.</span> <span class="comment">This</span> <span class="comment">method</span> <span class="comment">assumes</span> <span class="comment">that</span> <span class="comment">the</span> <span class="comment">training</span> <span class="comment">item</span> <span class="comment">has</span> <span class="comment">already</span> <span class="comment">been</span> <span class="comment">fed</span> <span class="comment">through</span> <span class="comment">the</span> <span class="comment">network</span><span class="comment">.</span>
319 <span class="comment">      * </span><span class="ST0">@param</span> <span class="comment">desiredOutput</span> <span class="comment">The</span> <span class="comment">desired</span> <span class="comment">output</span> <span class="comment">of</span> <span class="comment">the</span> <span class="comment">neural</span> <span class="comment">network</span>
320 <span class="comment">      * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">gradient</span> <span class="comment">of</span> <span class="comment">each</span> <span class="comment">parameter</span><span class="comment">&#39;</span><span class="comment">s</span> <span class="comment">error</span> <span class="comment">with</span> <span class="comment">respect</span> <span class="comment">to</span> <span class="comment">the</span> <span class="comment">loss</span> <span class="comment">function</span><span class="comment">.</span>
321       <span class="comment">*/</span>
322     <span class="literal">private</span> <span class="literal">double</span>[]calcGradientAttemptOptimised(<span class="literal">double</span>[]desiredOutput){
323         <span class="literal">double</span>[]errorInLayer= <span class="literal">new</span> <span class="literal">double</span>[1];<span class="comment">//stores the error in the current layer</span>
324         <span class="literal">double</span>[]errorInLayerAbove=<span class="literal">new</span> <span class="literal">double</span>[1];<span class="comment">//stores the error in the layer above</span>
325         <span class="literal">double</span> sumOfErrorAndWeights;<span class="comment">//stores the sum of the error multiplied by the weigths in a give layer</span>
326         <span class="literal">double</span> grad[]=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_LENGTH];<span class="comment">//stores the error gradient for each weight and bias in the network</span>
327         <span class="comment">//stores various indices used during the backpropogation process</span>
328         <span class="literal">int</span> indexWeightErrorChange;
329         <span class="literal">int</span> indexForActivsError;
330         <span class="literal">int</span> indexForAllGrad;
331         <span class="literal">int</span> changeD;
332         <span class="literal">int</span> totNeuronsExplored;
333         <span class="literal">int</span> indexForActivsGrad;
334         <span class="literal">int</span> activIndex;
335         <span class="literal">for</span>(<span class="literal">int</span> layerIndex=<span class="literal">this</span>.NET_STRUCTURE_LENGTH_MINUS_ONE;layerIndex&gt;0;layerIndex--){<span class="comment">//loops through the entire network</span>
336             errorInLayer=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.NET_STRUCTURE[layerIndex]];<span class="comment">//stores the error of each node in the given layer</span>
337             <span class="literal">if</span>(layerIndex==<span class="literal">this</span>.NET_STRUCTURE_LENGTH_MINUS_ONE){<span class="comment">//for the output layer, the error of each neuron is calculated</span>
338                 activIndex=<span class="literal">this</span>.ACTIV_IN;
339                 <span class="literal">for</span>(<span class="literal">int</span> neuronInLayer=0;neuronInLayer&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex];neuronInLayer++){
340                     errorInLayer[neuronInLayer]= <span class="literal">this</span>.nodeValuePostActivation[activIndex] - desiredOutput[neuronInLayer];<span class="comment">//this calcualtes the error of the network when a softmax activation is sued in conjunction with categorical cross entropoy loss</span>
341                     <span class="comment">//errorInLayer[neuronInLayer]=(this.nodeValuePostActivation[activIndex]-desiredOutput[neuronInLayer])*gradTanH(this.nodeValuePreActivation[activIndex]);//this is the method used to calculate the error of the network when using the mean squared error loss</span>
342                     activIndex++;
343                 }
344             }<span class="literal">else</span>{
345                 indexForActivsError=<span class="literal">this</span>.INDEX_FOR_ACTIVS_ERROR[layerIndex];
346                 changeD=1+<span class="literal">this</span>.NET_STRUCTURE[layerIndex];
347                 <span class="comment">//loops through each neuron in the current layer and calculates the error of the weights and biase of the neurons</span>
348                 <span class="literal">for</span>(<span class="literal">int</span> neuronInLayer = 0;neuronInLayer&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex];neuronInLayer++){
349                     <span class="literal">if</span>(NEURONS_DROPPED[neuronInLayer+INDEX_CACHE[layerIndex]]){
350                         errorInLayer[neuronInLayer]=0;
351                         indexForActivsError++;
352                         <span class="literal">continue</span>;
353                     }
354                     indexWeightErrorChange=<span class="literal">this</span>.INDEX_FOR_ALL_WEIGHTS_ERROR[layerIndex]+1+neuronInLayer;<span class="comment">//first index of weight</span>
355                     sumOfErrorAndWeights=0;<span class="comment">//sums the weights mulitplied by the error in the layer above</span>
356                     <span class="literal">for</span>(<span class="literal">int</span> neuronInLayerAfter=0;neuronInLayerAfter&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex+1];neuronInLayerAfter++){
357                         sumOfErrorAndWeights=sumOfErrorAndWeights+(errorInLayerAbove[neuronInLayerAfter]*<span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES[indexWeightErrorChange]);
358                         indexWeightErrorChange=indexWeightErrorChange+changeD;
359                     }
360                     errorInLayer[neuronInLayer]=sumOfErrorAndWeights*activationDerivative(<span class="literal">this</span>.nodeValuePreActivation[indexForActivsError]);<span class="comment">//stores the error of the neuron by muliplying the sum of the error and wieghts for the neuron by the pre activation node input</span>
361                     indexForActivsError++;
362                 }
363             }
364             <span class="comment">//errorInLayer has been generated - now the error of each bias and weight needs to be determined</span>
365             indexForAllGrad=<span class="literal">this</span>.CURRENT_ALL[layerIndex];
366             <span class="literal">for</span>(<span class="literal">int</span> neuronInLayer=0;neuronInLayer&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex];neuronInLayer++){<span class="comment">//loops through all neurons in layer</span>
367                 <span class="literal">if</span>(errorInLayer[neuronInLayer]==0){
368                     grad[indexForAllGrad]=0;indexForAllGrad++;
369                     <span class="literal">for</span>(<span class="literal">int</span> neuronInLayerBefore=0;neuronInLayerBefore&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex-1];neuronInLayerBefore++){<span class="comment">//generates error for weights</span>
370                         grad[indexForAllGrad]=0;<span class="comment">//calculates error for weight</span>
371                         indexForAllGrad++;
372                     }
373                     <span class="literal">continue</span>;
374                 }
375                 grad[indexForAllGrad]=errorInLayer[neuronInLayer];<span class="comment">//sets grad for bias (which is just equal to the error of the node)</span>
376                 indexForAllGrad++;
377                 indexForActivsGrad=<span class="literal">this</span>.TOT_NEURON[layerIndex];<span class="comment">//stores the index to use for POST_ACTIVATION array access</span>
378                 <span class="literal">for</span>(<span class="literal">int</span> neuronInLayerBefore=0;neuronInLayerBefore&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex-1];neuronInLayerBefore++){<span class="comment">//generates error for weights</span>
379                     grad[indexForAllGrad]=errorInLayer[neuronInLayer]*(<span class="literal">this</span>.nodeValuePostActivation[indexForActivsGrad]);<span class="comment">//calculates error for weight</span>
380                     indexForActivsGrad++;
381                     indexForAllGrad++;
382                 }
383             }
384             <span class="literal">if</span>(layerIndex==1){<span class="comment">//added in for optimisation</span>
385                 <span class="literal">return</span> grad;
386             }
387             <span class="comment">//sets the error in the current layer as the error in the layer above (this is used when calculating the error of the next layer)</span>
388             errorInLayerAbove=<span class="literal">new</span> <span class="literal">double</span>[errorInLayer.length];
389             <span class="literal">for</span>(<span class="literal">int</span> i =0;i&lt;errorInLayer.length;i++){
390                 errorInLayerAbove[i]=errorInLayer[i];
391             }
392         }
393         <span class="literal">return</span> grad;<span class="comment">//returns the gradients</span>
394     }
395     <span class="comment">/**</span>
396 <span class="comment">     * </span><span class="ST0">Shuffles</span> <span class="ST0">the</span> <span class="ST0">training</span> <span class="ST0">data</span> <span class="ST0">array</span><span class="ST0">.</span>
397 <span class="comment">     * </span><span class="ST0">@param</span> <span class="comment">array</span> <span class="comment">The</span> <span class="comment">array</span> <span class="comment">to</span> <span class="comment">shuffle</span>
398      <span class="comment">*/</span>
399      <span class="literal">private</span> <span class="literal">static</span> <span class="literal">void</span> shuffleTrainingDataArray(<span class="literal">int</span>[][]array){
400         Random rnd = <span class="literal">new</span> Random();<span class="literal">int</span> in;
401         <span class="literal">int</span> buff;
402         <span class="literal">for</span>(<span class="literal">int</span> y=0;y&lt;array.length;y++){<span class="comment">//loops through every row in array and swaps it with another row randomly</span>
403             in = rnd.nextInt(array.length);
404             <span class="literal">for</span>(<span class="literal">int</span> x=0;x&lt;array[0].length;x++){
405                 buff=array[y][x];
406                 array[y][x]=array[in][x];
407                 array[in][x]=buff;
408             }
409         }
410         <span class="comment">//as objects are passed by reference in java, the array is shuffled without needing to return it</span>
411     }
412      <span class="comment">/**</span>
413 <span class="comment">      * </span><span class="ST0">Trains</span> <span class="ST0">the</span> <span class="ST0">neural</span> <span class="ST0">network</span> <span class="ST0">on</span> <span class="ST0">a</span> <span class="ST0">dataset</span> <span class="ST0">using</span> <span class="ST0">backpropagation</span> <span class="ST0">in</span> <span class="ST0">conjuction</span> <span class="ST0">with</span> <span class="ST0">the</span> <span class="ST0">ADAM</span> <span class="ST0">optimiser</span><span class="ST0">.</span>
414 <span class="comment">      * </span><span class="ST0">@param</span> <span class="comment">LEARNING_RATE</span> <span class="comment">The</span> <span class="comment">learning</span> <span class="comment">rate</span> <span class="comment">of</span> <span class="comment">the</span> <span class="comment">network</span><span class="comment"> - </span><span class="comment">this</span> <span class="comment">is</span> <span class="comment">recommended</span> <span class="comment">to</span> <span class="comment">be</span><span class="comment"> 0</span><span class="comment">.</span><span class="comment">001 </span><span class="comment">as</span> <span class="comment">the</span> <span class="comment">ADAM</span> <span class="comment">optimiser</span> <span class="comment">is</span> <span class="comment">used</span><span class="comment">.</span>
415 <span class="comment">      * </span><span class="ST0">@param</span> <span class="comment">NO_OF_EPOCHS</span> <span class="comment">The</span> <span class="comment">number</span> <span class="comment">of</span> <span class="comment">epochs</span> <span class="comment">in</span> <span class="comment">the</span> <span class="comment">training</span> <span class="comment">cycle</span><span class="comment"> (</span><span class="comment">each</span> <span class="comment">epoch</span> <span class="comment">is</span> <span class="comment">one</span> <span class="comment">complete</span> <span class="comment">pass</span> <span class="comment">of</span> <span class="comment">the</span> <span class="comment">dataset</span><span class="comment">)</span><span class="comment">.</span>
416 <span class="comment">      * </span><span class="ST0">@param</span> <span class="comment">SAMPLE_SIZE</span> <span class="comment">The</span> <span class="comment">batch</span> <span class="comment">size</span><span class="comment"> - </span><span class="comment">the</span> <span class="comment">number</span> <span class="comment">of</span> <span class="comment">training</span> <span class="comment">items</span> <span class="comment">used</span> <span class="comment">per</span> <span class="comment">network</span> <span class="comment">update</span><span class="comment">.</span>
417 <span class="comment">      * </span><span class="ST0">@param</span> <span class="comment">TOTAL_TRAIN_SIZE</span> <span class="comment">The</span> <span class="comment">total</span> <span class="comment">size</span> <span class="comment">of</span> <span class="comment">the</span> <span class="comment">training</span> <span class="comment">data</span>
418 <span class="comment">      * </span><span class="ST0">@param</span> <span class="comment">PRINT_AND_TEST_AFTER_THIS_MANY_ITERATIONS</span> <span class="comment">How</span> <span class="comment">many</span> <span class="comment">iterations</span> <span class="comment">to</span> <span class="comment">test</span> <span class="comment">the</span> <span class="comment">accuracy</span> <span class="comment">and</span> <span class="comment">print</span> <span class="comment">it</span><span class="comment">.</span> <span class="comment">Each</span> <span class="comment">iteration</span> <span class="comment">is</span> <span class="comment">one</span> <span class="comment">parameter</span> <span class="comment">update</span> <span class="comment">of</span> <span class="comment">the</span> <span class="comment">network</span><span class="comment">.</span>
419 <span class="comment">     * </span><span class="ST0">@param</span> <span class="comment">DROPOUT_RATE</span> <span class="comment">The</span> <span class="comment">dropout</span> <span class="comment">rate</span> <span class="comment">of</span> <span class="comment">each</span> <span class="comment">neuron</span>
420       <span class="comment">*/</span>
421      <span class="literal">public</span> <span class="literal">void</span> trainNew( <span class="literal">final</span> <span class="literal">double</span> LEARNING_RATE, <span class="literal">final</span> <span class="literal">int</span> NO_OF_EPOCHS,<span class="literal">final</span> <span class="literal">int</span> SAMPLE_SIZE,<span class="literal">final</span> <span class="literal">int</span> TOTAL_TRAIN_SIZE,<span class="literal">final</span> <span class="literal">int</span> PRINT_AND_TEST_AFTER_THIS_MANY_ITERATIONS, <span class="literal">final</span> <span class="literal">double</span> DROPOUT_RATE){
422         <span class="literal">double</span> changeBy[];<span class="comment">//stores how much to change each parameter by</span>
423         <span class="literal">final</span> <span class="literal">long</span> TIME_BEFORE=System.currentTimeMillis();<span class="comment">//used to calculate how long the network has been running for</span>
424         <span class="literal">double</span> timeTemp = System.currentTimeMillis();<span class="comment">//used to calculate the time between each testing of the network</span>
425         <span class="literal">final</span> <span class="literal">int</span> NO_OF_ITERATIONS_IN_ONE_EPOCH=(TOTAL_TRAIN_SIZE/SAMPLE_SIZE);<span class="comment">//stores number of iterations per epoch</span>
426         <span class="literal">final</span> <span class="literal">double</span> SAMPLE_SIZE_DOUBLE=Double.parseDouble(String.valueOf(SAMPLE_SIZE));
427         <span class="literal">double</span>[]bestNet = <span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_LENGTH];<span class="comment">//stores the best network configuration</span>
428         <span class="literal">double</span> accuracy;<span class="comment">//stores the accuracy of the network</span>
429         <span class="literal">double</span> time = System.currentTimeMillis();
430         <span class="literal">double</span> mT[]=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_LENGTH];<span class="comment">//stores the mT values - part of the ADAM optimisation formula</span>
431         <span class="literal">double</span> vT[]=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_LENGTH];<span class="comment">//stores the vT values - part of the ADAM optimisation formula</span>
432         <span class="literal">double</span> mHatT[]=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_LENGTH];<span class="comment">//stores the mHatT values - part of the ADAM optimisation formula</span>
433         <span class="literal">double</span> vHatT[]=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_LENGTH];<span class="comment">//stores the vHatT values - part of the ADAM optimisation formula</span>
434         <span class="literal">double</span> prevMT[]=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_LENGTH];<span class="comment">// stores the pervious MT values - part of the ADAM optimisation formula</span>
435         <span class="literal">double</span> prevVT[]=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_LENGTH];<span class="comment">// stores the pervious VT values - part of the ADAM optimisation formula</span>
436         <span class="literal">double</span> t =0.0;<span class="comment">//stores the time step - also used in the ADAM forumla</span>
437         <span class="literal">try</span>{<span class="comment">//checks to see if any previous learning iterations have been run with this model and allows for ADAM training to continue</span>
438             <span class="literal">double</span> l =<span class="literal">this</span>.previousVHat[0];
439             <span class="literal">for</span>(<span class="literal">int</span> i =0;i&lt;<span class="literal">this</span>.previousMHat.length;i++){
440                 prevMT[i]=<span class="literal">this</span>.previousMHat[i];
441                 prevVT[i]=<span class="literal">this</span>.previousVHat[i];    
442             }
443             t=<span class="literal">this</span>.timeStep;
444         }<span class="literal">catch</span>(java.lang.NullPointerException e){
445         }
446         <span class="comment">//stores constants used in ADAM optimisation method</span>
447         <span class="literal">final</span> <span class="literal">double</span> BETA_ONE=0.9;
448         <span class="literal">final</span> <span class="literal">double</span> BETA_TWO=0.999;
449         Random rnd = <span class="literal">new</span> Random();
450         <span class="literal">final</span> <span class="literal">double</span> EPSILON = Math.pow(10,-8);
451         <span class="literal">double</span> gradOfOne[];<span class="comment">//stores the gradient of one item of training data</span>
452         <span class="literal">int</span> indexOfTrain=0;<span class="comment">//stores the index that accesses the appropriate training data</span>
453         <span class="literal">int</span> testData[][]= PhilipMCS5SoftwareDevelopmentNeuralNetworkTrain.getTestData();<span class="comment">//laods the test data</span>
454         <span class="literal">int</span>[][]trainingData=PhilipMCS5SoftwareDevelopmentNeuralNetworkTrain.getTrainingData();<span class="comment">//loads the training data</span>
455         <span class="literal">int</span> indexPrintAndTest=1;<span class="comment">//stores the the number of iterations since the previous print of accuracy</span>
456         <span class="literal">double</span> start[]=<span class="literal">this</span>.testAccuracy(testData);<span class="comment">//calculates the initial network accuracy before any training</span>
457         <span class="literal">double</span> highestAcc=start[0];<span class="comment">//stores the highest network accuracy</span>
458         <span class="literal">for</span>(<span class="literal">int</span> i =0;i&lt;<span class="literal">this</span>.ALL_LENGTH;i++){
459             bestNet[i]=<span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES[i];<span class="comment">//initialises the best network as the current network</span>
460         }
461         <span class="literal">double</span> desiredOut[]= <span class="literal">new</span> <span class="literal">double</span>[2];<span class="comment">//stores the desired output of the neural network</span>
462         <span class="literal">double</span>[]inputToNet=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.NET_STRUCTURE[0]];<span class="comment">//stores the array used to make an input to the net</span>
463         <span class="literal">int</span> indexData=0;<span class="comment">//stores the index of training data to be accessed within an iteration</span>
464         System.out.println(<span class="string">&quot;</span><span class="string">Accuracy before training </span><span class="string">&quot;</span>+highestAcc+<span class="string">&quot;</span><span class="string"> %</span><span class="string">&quot;</span>+<span class="string">&quot;</span><span class="string"> loss of </span><span class="string">&quot;</span>+start[1]);
465         <span class="literal">for</span>(<span class="literal">int</span> epoch=1;epoch&lt;=NO_OF_EPOCHS;epoch++){<span class="comment">//performs the specified number of epochs</span>
466                 shuffleTrainingDataArray(trainingData);<span class="comment">//shuffles the training data at the start of every epoch</span>
467             indexOfTrain=0;
468             <span class="literal">for</span>(<span class="literal">int</span> iteration=1;iteration&lt;=NO_OF_ITERATIONS_IN_ONE_EPOCH;iteration++){<span class="comment">//performs every iteration in the epoch</span>
469                 changeBy = <span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_LENGTH];<span class="comment">//sets change by to an empty array of the appropriate length</span>
470                 indexData=indexOfTrain;
471                 indexOfTrain=indexOfTrain+SAMPLE_SIZE;
472                 <span class="literal">for</span> (<span class="literal">int</span> sampleNo = 0; sampleNo &lt; SAMPLE_SIZE; sampleNo++) {<span class="comment">//loops thorugh every data sample in the iteration</span>
473                     <span class="literal">for</span>(<span class="literal">int</span> i=0;i&lt;<span class="literal">this</span>.NET_STRUCTURE[0];i++){<span class="comment">//loads the network input</span>
474                         inputToNet[i]=trainingData[indexData][i+2];
475                     }
476                     <span class="comment">//drops out neurons for regularisation if appropriate</span>
477                     <span class="literal">for</span>(<span class="literal">int</span> node=0;node&lt;LENGTH_OF_NEURONS_DROPPED_ARRAY;node++){
478                         NEURONS_DROPPED[node]=rnd.nextFloat()&lt;DROPOUT_RATE;
479                     }
480                     <span class="literal">this</span>.feedThroughNet(inputToNet);<span class="comment">//feeds the input through the network</span>
481                     desiredOut[0]=trainingData[indexData][0];<span class="comment">//stores the desired network output</span>
482                     desiredOut[1]=trainingData[indexData][1];
483                     indexData++;
484                     gradOfOne = <span class="literal">this</span>.calcGradientAttemptOptimised(desiredOut);<span class="comment">//calculates the error of each parameter</span>
485                     <span class="literal">for</span> (<span class="literal">int</span> i = 0; i &lt; <span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES.length; i++) {<span class="comment">//stores how much each paramater should be changed by</span>
486                         changeBy[i]=changeBy[i]+gradOfOne[i];
487                     }
488                 }
489                 t=t+1.0;<span class="comment">//increments the time step - used in the ADAM formula</span>
490                 <span class="literal">for</span>(<span class="literal">int</span> i=0;i&lt;changeBy.length;i++){<span class="comment">//updates all weights and biases using the ADAM optimisation technique</span>
491                     changeBy[i]=changeBy[i]/SAMPLE_SIZE_DOUBLE;<span class="comment">//change by is normalised to reflect the sample size</span>
492                     <span class="comment">//the ADAM optimisation formula is implemented</span>
493                     mT[i]=BETA_ONE*prevMT[i]+(1.0-BETA_ONE) * changeBy[i];
494                     vT[i]=BETA_TWO*prevVT[i]+(1.0-BETA_TWO)*(changeBy[i]*changeBy[i]);
495                     mHatT[i]=mT[i]/(1.0-Math.pow(BETA_ONE,t));
496                     vHatT[i]=vT[i]/(1.0-Math.pow(BETA_TWO,t));
497                     prevMT[i]=mT[i];
498                     prevVT[i]=vT[i];
499                     <span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES[i]=<span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES[i] - (LEARNING_RATE*mHatT[i]/(Math.sqrt(vHatT[i])+EPSILON));<span class="comment">//the network values are configured appropriately  </span>
500                    <span class="comment">//this.allWeightsAndBiases[i]=this.allWeightsAndBiases[i]-(changeBy[i]*learningRate); // this method is the commented out but is the gradient descent formula</span>
501                 }
502                 <span class="literal">if</span>(indexPrintAndTest==PRINT_AND_TEST_AFTER_THIS_MANY_ITERATIONS){<span class="comment">//checks to see if they system should test the network and print the result of this</span>
503                     <span class="comment">//tests network, displays this and updates the best accuracy and best network variables if appropriate</span>
504                     System.out.println(<span class="string">&quot;</span><span class="string">Time training between training </span><span class="string">&quot;</span>+((System.currentTimeMillis()-time)/1000.0)+<span class="string">&quot;</span><span class="string"> seconds</span><span class="string">&quot;</span>);
505                     <span class="comment">//ensures that no nodes are dropped when testing accuracy</span>
506                     <span class="literal">for</span>(<span class="literal">int</span> n=0;n&lt;LENGTH_OF_NEURONS_DROPPED_ARRAY;n++){
507                         NEURONS_DROPPED[n]=<span class="literal">false</span>;
508                     }
509                     <span class="literal">double</span>[]accAndLoss=<span class="literal">this</span>.testAccuracy(testData);
510                     time = System.currentTimeMillis();
511                     accuracy = accAndLoss[0];
512                     <span class="literal">if</span> (accuracy &gt;= highestAcc || highestAcc &lt;= 0) {
513                         <span class="literal">for</span> (<span class="literal">int</span> i = 0; i &lt; <span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES.length; i++) {
514                             bestNet[i] = <span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES[i];
515                         }
516                         highestAcc = accuracy;
517                     }
518                     System.out.println(<span class="string">&quot;</span><span class="string">Accuracy </span><span class="string">&quot;</span> + accuracy + <span class="string">&quot;</span><span class="string">% after </span><span class="string">&quot;</span> + epoch + <span class="string">&quot;</span><span class="string"> epochs out of </span><span class="string">&quot;</span> + NO_OF_EPOCHS + <span class="string">&quot;</span><span class="string"> epochs. Sample size of </span><span class="string">&quot;</span> + SAMPLE_SIZE + <span class="string">&quot;</span><span class="string"> with a learning rate of </span><span class="string">&quot;</span> + LEARNING_RATE+<span class="string">&quot;</span><span class="string"> loss of </span><span class="string">&quot;</span>+accAndLoss[1]+ <span class="string">&quot;</span><span class="string">iteration </span><span class="string">&quot;</span>+(iteration)+<span class="string">&quot;</span><span class="string"> out of </span><span class="string">&quot;</span>+NO_OF_ITERATIONS_IN_ONE_EPOCH+<span class="string">&quot;</span><span class="string"> best accuracy of </span><span class="string">&quot;</span>+highestAcc);
519                     indexPrintAndTest=0;
520                     <span class="literal">if</span>((System.currentTimeMillis()-timeTemp)/1000.0&gt;=36000){<span class="comment">//this clause is used to ensure that the network only takes a certain amount of time to train</span>
521                         System.out.println(<span class="string">&quot;</span><span class="string">ending training as time has elapsed </span><span class="string">&quot;</span>+ ((System.currentTimeMillis()-timeTemp)/1000.0));
522                         epoch=NO_OF_EPOCHS+1;
523                         <span class="literal">break</span>;
524                     } 
525                 }
526                 indexPrintAndTest++;
527             }
528         }
529         <span class="comment">//stores the values of the ADAM technique - this allows the same neural network to call the train method multiple times</span>
530         <span class="literal">this</span>.timeStep=t;
531         <span class="literal">this</span>.previousMHat=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_LENGTH];
532         <span class="literal">this</span>.previousVHat=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.ALL_LENGTH];
533         <span class="literal">for</span>(<span class="literal">int</span> i =0;i&lt;prevMT.length;i++){
534             <span class="literal">this</span>.previousMHat[i]=prevMT[i];
535             <span class="literal">this</span>.previousVHat[i]=prevVT[i];
536         }
537         <span class="comment">//calclates the final network accuracy</span>
538         accuracy = <span class="literal">this</span>.testAccuracy(testData)[0];
539         System.out.println(<span class="string">&quot;</span><span class="string">Accuracy </span><span class="string">&quot;</span>+accuracy+<span class="string">&quot;</span><span class="string"> with loss of </span><span class="string">&quot;</span>+<span class="literal">this</span>.testAccuracy(testData)[1]);
540         <span class="comment">//writes all network details to a text file</span>
541         <span class="literal">long</span> timeAtEnd = System.currentTimeMillis();
542         <span class="literal">long</span> timeTaken=timeAtEnd-TIME_BEFORE;
543         <span class="literal">try</span>{
544             FileWriter writer = <span class="literal">new</span> FileWriter(FILE_NAME,<span class="literal">true</span>);
545             BufferedWriter bufferedWriter = <span class="literal">new</span> BufferedWriter(writer);
546             bufferedWriter.write(<span class="string">&quot;</span><span class="string">----------------------------</span><span class="string">&quot;</span>);
547             bufferedWriter.newLine();
548             bufferedWriter.write(<span class="string">&quot;</span><span class="string">Network Structure </span><span class="string">&quot;</span>);
549             <span class="literal">for</span>(<span class="literal">int</span> i =0;i&lt;<span class="literal">this</span>.NET_STRUCTURE.length;i++){
550                 bufferedWriter.write(String.valueOf(<span class="literal">this</span>.NET_STRUCTURE[i])+<span class="string">&quot;</span>  <span class="string">&quot;</span>);
551             }
552             bufferedWriter.newLine();
553             bufferedWriter.write(<span class="string">&quot;</span><span class="string">All weights and biases</span><span class="string">&quot;</span>);
554             bufferedWriter.newLine();
555             <span class="literal">for</span>(<span class="literal">int</span> i =0;i&lt;<span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES.length;i++){
556                 bufferedWriter.write(String.valueOf(<span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES[i]));
557                 bufferedWriter.newLine();
558             }
559             bufferedWriter.write(<span class="string">&quot;</span><span class="string">No of epochs </span><span class="string">&quot;</span>+NO_OF_EPOCHS);
560             bufferedWriter.newLine();
561             bufferedWriter.write(<span class="string">&quot;</span><span class="string">Final accuracy of </span><span class="string">&quot;</span>+accuracy);
562             bufferedWriter.newLine();
563             bufferedWriter.write(<span class="string">&quot;</span><span class="string">time taken in milliseconds </span><span class="string">&quot;</span>+timeTaken);
564             bufferedWriter.newLine();
565             bufferedWriter.write(<span class="string">&quot;</span><span class="string">Learning rate of </span><span class="string">&quot;</span>+LEARNING_RATE);
566             bufferedWriter.newLine();
567             bufferedWriter.write(<span class="string">&quot;</span><span class="string">Best network (with accuracy </span><span class="string">&quot;</span>+highestAcc+<span class="string">&quot;</span><span class="string">% with this sturcture had a configuration of</span><span class="string">&quot;</span>);
568             bufferedWriter.newLine();
569             <span class="literal">for</span>(<span class="literal">int</span> i=0;i&lt;bestNet.length;i++){
570                 bufferedWriter.write(String.valueOf(bestNet[i]));
571                 bufferedWriter.newLine();
572             }
573             bufferedWriter.newLine();
574             bufferedWriter.write(<span class="string">&quot;</span><span class="string">Using a sample size of </span><span class="string">&quot;</span>+SAMPLE_SIZE+<span class="string">&quot;</span><span class="string"> for each epoch out of a total of 60000 samples. Note this doesnt neccesarly apply if this is true:  false</span><span class="string">&quot;</span>);
575             bufferedWriter.newLine();
576             bufferedWriter.write(<span class="string">&quot;</span><span class="string">----------------------------</span><span class="string">&quot;</span>);
577             bufferedWriter.close();
578             writer.close();
579         }<span class="literal">catch</span>(IOException e){
580             System.out.println(<span class="string">&quot;</span><span class="string">error with file </span><span class="string">&quot;</span>+FILE_NAME+<span class="string">&quot;</span><span class="string"> error of </span><span class="string">&quot;</span>+e);
581             System.out.println(<span class="string">&quot;</span><span class="string">Error trace</span><span class="string">&quot;</span>);
582             e.printStackTrace();
583         }
584     }
585      <span class="comment">/**</span>
586 <span class="comment">      * </span><span class="ST0">Gets</span> <span class="ST0">the</span> <span class="ST0">training</span> <span class="ST0">data</span>
587 <span class="comment">      * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">training</span> <span class="comment">data</span>
588       <span class="comment">*/</span>
589     <span class="literal">private</span> <span class="literal">static</span> <span class="literal">int</span>[][]getTrainingData(){
590         <span class="literal">return</span> PhilipMCS5SoftwareDevelopmentNeuralNetworkTrain.getTrainingData();
591     }
592     <span class="comment">/**</span>
593 <span class="comment">     * </span><span class="ST0">Gets</span> <span class="ST0">the</span> <span class="ST0">test</span> <span class="ST0">data</span>
594 <span class="comment">     * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">test</span> <span class="comment">data</span>
595      <span class="comment">*/</span>
596     <span class="literal">private</span> <span class="literal">static</span> <span class="literal">int</span>[][]getTestData(){
597         <span class="literal">return</span> PhilipMCS5SoftwareDevelopmentNeuralNetworkTrain.getTestData();      
598     }
599     <span class="comment">/**</span>
600 <span class="comment">     * </span><span class="ST0">Tests</span> <span class="ST0">the</span> <span class="ST0">network</span><span class="ST0">&#39;</span><span class="ST0">s</span> <span class="ST0">accuracy</span>
601 <span class="comment">     * </span><span class="ST0">@param</span> <span class="comment">testData</span> <span class="comment">The</span> <span class="comment">test</span> <span class="comment">data</span> <span class="comment">to</span> <span class="comment">use</span> <span class="comment">when</span> <span class="comment">testing</span> <span class="comment">accuracy</span>
602 <span class="comment">     * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">accuracy</span> <span class="comment">and</span> <span class="comment">loss</span> <span class="comment">of</span> <span class="comment">the</span> <span class="comment">system</span> <span class="comment">as</span> <span class="comment">an</span> <span class="comment">array</span> <span class="comment">of</span> <span class="comment">length</span><span class="comment"> 2</span><span class="comment">.</span> <span class="comment">Element</span><span class="comment"> 0 </span><span class="comment">corresponds</span> <span class="comment">to</span> <span class="comment">the</span> <span class="comment">accuracy</span><span class="comment">, </span><span class="comment">whilst</span> <span class="comment">element</span><span class="comment"> 1 </span><span class="comment">is</span> <span class="comment">the</span> <span class="comment">loss</span><span class="comment">.</span>
603      <span class="comment">*/</span>
604     <span class="literal">public</span> <span class="literal">double</span>[]testAccuracy(<span class="literal">int</span> testData[][]){
605         <span class="literal">double</span> time = System.currentTimeMillis();<span class="comment">//used to calculate how long it takes to test the system</span>
606         <span class="literal">int</span> totalCorrect=0;<span class="literal">double</span>[]out;
607         <span class="literal">double</span>[]desiredOut = <span class="literal">new</span> <span class="literal">double</span>[2];
608         <span class="literal">double</span> averageLoss=0.0;<span class="literal">int</span> greatestIndex;<span class="literal">double</span> err;
609         <span class="literal">double</span> diff;
610         <span class="literal">double</span>[]netInput=<span class="literal">new</span> <span class="literal">double</span>[PhilipMCS5SoftwareDevelopmentNeuralNetworkTrain.WIDTH_OF_DATA-2];<span class="comment">//stores the network input</span>
611         <span class="literal">for</span>(<span class="literal">int</span> y=0;y&lt;testData.length;y++){<span class="comment">//loops trhough every item in the test dataset</span>
612             desiredOut[0]=testData[y][0];<span class="comment">//gets the desired output of the neural network</span>
613             desiredOut[1]=testData[y][1];
614             <span class="literal">for</span>(<span class="literal">int</span> x=2;x&lt;testData[0].length;x++){<span class="comment">//generates the input to the neural network</span>
615                 netInput[x-2]=testData[y][x];
616             }
617             out=<span class="literal">this</span>.feedThroughNet(netInput);<span class="comment">//passes the network input throught the neural network</span>
618             greatestIndex=0;<span class="comment">//determines whether out[0] or out[1] is biggest</span>
619             <span class="literal">if</span>(out[1]&gt;out[0]){
620                 greatestIndex=1;
621             }
622             <span class="literal">if</span>(desiredOut[greatestIndex]==1.0){<span class="comment">//checks to see if network is correct</span>
623                 totalCorrect++;
624             }
625            <span class="comment">/* diff=out[0]-desiredOut[0];</span>
626 <span class="comment">            err=diff*diff;*/</span>
627            err=0;
628             <span class="comment">//calculates the loss of the network</span>
629             <span class="literal">for</span>(<span class="literal">int</span> i=0;i&lt;out.length;i++){
630                 err = err+(desiredOut[i]*Math.log(out[i]));<span class="comment">//categorical cross entropy loss is employed -  this makes the assumption that the desired output is a one hot vector</span>
631                 
632             }err=err*-1.0;
633             averageLoss=averageLoss+(err);<span class="comment">//adds loss to total loss</span>
634         }
635         
636         <span class="literal">double</span> accuracy = (<span class="literal">double</span>)totalCorrect/(<span class="literal">double</span>)PhilipMCS5SoftwareDevelopmentNeuralNetworkTrain.NO_OF_TEST_BOARDS;<span class="comment">//calculates the accuracy of the network</span>
637         accuracy= accuracy*100.0;
638         averageLoss=averageLoss/(<span class="literal">double</span>)PhilipMCS5SoftwareDevelopmentNeuralNetworkTrain.NO_OF_TEST_BOARDS;<span class="comment">//calculates the average loss</span>
639         System.out.println(<span class="string">&quot;</span><span class="string">Testing time: </span><span class="string">&quot;</span>+((System.currentTimeMillis()-time)/1000.0)+<span class="string">&quot;</span><span class="string"> seconds . Total correct: </span><span class="string">&quot;</span>+totalCorrect+<span class="string">&quot;</span><span class="string"> out of </span><span class="string">&quot;</span>+PhilipMCS5SoftwareDevelopmentNeuralNetworkTrain.NO_OF_TEST_BOARDS);
640         <span class="literal">double</span> ret[]={accuracy,averageLoss};
641         <span class="literal">return</span> ret;
642     }
643     <span class="comment">/**</span>
644 <span class="comment">     * </span><span class="ST0">This</span> <span class="ST0">feeds</span> <span class="ST0">a</span> <span class="ST0">desired</span> <span class="ST0">input</span> <span class="ST0">through</span> <span class="ST0">the</span> <span class="ST0">neural</span> <span class="ST0">network</span> <span class="ST0">and</span> <span class="ST0">returns</span> <span class="ST0">the</span> <span class="ST0">output</span><span class="ST0">.</span> <span class="comment">This</span> <span class="comment">is</span> <span class="comment">sometimes</span> <span class="comment">referred</span> <span class="comment">to</span> <span class="comment">as</span> <span class="comment">forward</span> <span class="comment">propagation</span><span class="comment">.</span>
645 <span class="comment">     * </span><span class="ST0">@param</span> <span class="comment">input</span> <span class="comment">The</span> <span class="comment">input</span> <span class="comment">array</span> <span class="comment">to</span> <span class="comment">the</span> <span class="comment">network</span>
646 <span class="comment">     * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">neural</span> <span class="comment">network</span> <span class="comment">output</span>
647      <span class="comment">*/</span>
648     <span class="literal">public</span> <span class="literal">double</span>[]feedThroughNet(<span class="literal">double</span>[]input){
649         <span class="literal">int</span> currentAllIndex=0;<span class="comment">//stores the current index used to access all weights and biases of the neural network</span>
650         <span class="literal">double</span> []nodeValuesAfterActivation;<span class="comment">//stores the activations in the current layer</span>
651         <span class="literal">double</span> []out = <span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.NET_STRUCTURE[<span class="literal">this</span>.NET_STRUCTURE_LENGTH_MINUS_ONE]];<span class="comment">//stores the output of the neural network</span>
652         <span class="literal">int</span> indexForNodesInThisLayer;<span class="comment">//stores the index used to access the nodes in a given layer</span>
653         <span class="literal">double</span> valueOfBias;<span class="comment">//stores the value of the current bias</span>
654         <span class="literal">double</span> nodeValPreActivation;<span class="comment">//stores the value of the node before the activation function is applied to it</span>
655         <span class="literal">double</span>[]nodeValuesAfterActivationPrevLayer=input;<span class="comment">//stores the value of activated nodes in the previous layer</span>
656         <span class="literal">int</span> indexForPreActivs=0;<span class="comment">//stores the index for pre activation arrays</span>
657         <span class="literal">int</span> indexForPostActivs=0;<span class="comment">//stores the index for post actviation array</span>
658         <span class="literal">this</span>.nodeValuePreActivation=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.TOTAL_NEURONS];
659         <span class="literal">this</span>.nodeValuePostActivation=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.TOTAL_NEURONS];
660         <span class="literal">for</span>(<span class="literal">int</span> i=0;i&lt;<span class="literal">this</span>.NET_STRUCTURE[0];i++){<span class="comment">//sets values for the input layer - all nodes in input layer = input[i]</span>
661             <span class="literal">this</span>.nodeValuePreActivation[indexForPreActivs]=input[i];
662             <span class="literal">this</span>.nodeValuePostActivation[indexForPostActivs]=input[i];
663             indexForPostActivs++;
664             indexForPreActivs++;
665         }
666         <span class="literal">for</span>(<span class="literal">int</span> layerIndex=1;layerIndex&lt;<span class="literal">this</span>.NET_STRUCTURE_LENGTH;layerIndex++){<span class="comment">//loops through network and calculates values of each layer</span>
667             indexForNodesInThisLayer=0;
668             nodeValuesAfterActivation=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.NET_STRUCTURE[layerIndex]];
669             <span class="literal">for</span>(<span class="literal">int</span> indexOfLayer=0;indexOfLayer&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex];indexOfLayer++){<span class="comment">//neurons in current layer</span>
670                 <span class="literal">if</span>(layerIndex!=NET_STRUCTURE_LENGTH_MINUS_ONE&amp;&amp;NEURONS_DROPPED[INDEX_CACHE[layerIndex]+indexOfLayer]){<span class="comment">//check to see if node is dropped out</span>
671                     currentAllIndex=currentAllIndex+1+<span class="literal">this</span>.NET_STRUCTURE[layerIndex-1];
672                     <span class="literal">this</span>.nodeValuePreActivation[indexForPreActivs]=0;indexForPreActivs++;
673                     <span class="literal">this</span>.nodeValuePostActivation[indexForPostActivs]=0;indexForPostActivs++;
674                     nodeValuesAfterActivation[indexForNodesInThisLayer]=0;
675                     indexForNodesInThisLayer++;
676                     <span class="literal">continue</span>;
677                 }
678                 valueOfBias=<span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES[currentAllIndex];<span class="comment">//stores bias of node</span>
679                 currentAllIndex++;
680                 nodeValPreActivation=0;
681                 <span class="literal">for</span>(<span class="literal">int</span> prevLayerIndex=0;prevLayerIndex&lt;<span class="literal">this</span>.NET_STRUCTURE[layerIndex-1];prevLayerIndex++){<span class="comment">//sums wieghts and previous node values to get preActivation value</span>
682                     nodeValPreActivation=nodeValPreActivation + nodeValuesAfterActivationPrevLayer[prevLayerIndex] * <span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES[currentAllIndex];
683                     currentAllIndex++;
684 
685                 }
686                 <span class="literal">this</span>.nodeValuePreActivation[indexForPreActivs]=nodeValPreActivation+valueOfBias;<span class="comment">//sets value before actviation in this layer</span>
687                 indexForPreActivs++;
688                 <span class="literal">if</span>(layerIndex!=<span class="literal">this</span>.NET_STRUCTURE_LENGTH_MINUS_ONE){<span class="comment">//sets  the post activation value</span>
689                     nodeValuesAfterActivation[indexForNodesInThisLayer]=activation(<span class="literal">this</span>.nodeValuePreActivation[indexForPreActivs-1]);<span class="comment">//for non output layers the choosen actviation is used - normally this is reLu</span>
690                     indexForNodesInThisLayer++;
691                     <span class="literal">this</span>.nodeValuePostActivation[indexForPostActivs]=nodeValuesAfterActivation[indexForNodesInThisLayer-1];
692                     indexForPostActivs++;
693                 }<span class="literal">else</span> <span class="literal">if</span>(indexOfLayer==<span class="literal">this</span>.NET_STRUCTURE[layerIndex]-1){<span class="comment">//for the output layer, the softmax activation is used</span>
694                     <span class="literal">double</span> preSigLayer[]=<span class="literal">new</span> <span class="literal">double</span>[<span class="literal">this</span>.NET_STRUCTURE[layerIndex]];
695                     <span class="literal">for</span>(<span class="literal">int</span> indexVal=0;indexVal&lt;preSigLayer.length;indexVal++){
696                        preSigLayer[indexVal]=<span class="literal">this</span>.nodeValuePreActivation[<span class="literal">this</span>.TOTAL_NEURONS_MINUS_LAST_LAYER+indexVal];
697                     }
698                     nodeValuesAfterActivation=softmax(preSigLayer);
699                     <span class="literal">for</span>(<span class="literal">int</span> i=0;i&lt;nodeValuesAfterActivation.length;i++){
700                         <span class="literal">this</span>.nodeValuePostActivation[indexForPostActivs]=nodeValuesAfterActivation[i];
701                         indexForPostActivs++;
702                     }
703                 }
704                 <span class="comment">/*else{</span>
705 <span class="comment">                    nodeValuesAfterActivation[indexForNodesInThisLayer]=tanH(this.nodeValuePreActivation[indexForPreActivs-1]);//tanh used for output layer of non classification model</span>
706 <span class="comment">                    indexForNodesInThisLayer++;</span>
707 <span class="comment">                    this.nodeValuePostActivation[indexForPostActivs]=nodeValuesAfterActivation[indexForNodesInThisLayer-1];</span>
708 <span class="comment">                    indexForPostActivs++;                    </span>
709 <span class="comment">                }*/</span>
710             }
711             nodeValuesAfterActivationPrevLayer=nodeValuesAfterActivation;
712             <span class="literal">if</span>(layerIndex==<span class="literal">this</span>.NET_STRUCTURE_LENGTH_MINUS_ONE){
713                 out = nodeValuesAfterActivation;
714             }
715         }
716         <span class="comment">//returns the network output</span>
717         <span class="literal">return</span> out;
718     }
719     <span class="comment">/**</span>
720 <span class="comment">     * </span><span class="ST0">Sets</span> <span class="ST0">all</span> <span class="ST0">weights</span> <span class="ST0">and</span> <span class="ST0">biases</span> <span class="ST0">in</span> <span class="ST0">the</span> <span class="ST0">network</span> <span class="ST0">to</span> <span class="ST0">the</span> <span class="ST0">chosen</span> <span class="ST0">value</span>
721 <span class="comment">     * </span><span class="ST0">@param</span> <span class="comment">allWAndB</span> <span class="comment">The</span> <span class="comment">new</span> <span class="comment">set</span> <span class="comment">of</span> <span class="comment">weights</span> <span class="comment">and</span> <span class="comment">biases</span>
722      <span class="comment">*/</span>
723     <span class="literal">public</span> <span class="literal">void</span> setAllWeightsAndBiases(<span class="literal">double</span>[]allWAndB){
724         <span class="literal">if</span>(allWAndB.length!=<span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES.length){
725             System.out.println(<span class="string">&quot;</span><span class="string">Attempted to change weights and biases to an array of different length </span><span class="string">&quot;</span>);
726         }
727         <span class="literal">for</span>(<span class="literal">int</span> i =0;i&lt;allWAndB.length;i++){
728             <span class="literal">this</span>.ALL_WEIGHTS_AND_BIASES[i]=allWAndB[i];
729         }
730     }
731     <span class="comment">/**</span>
732 <span class="comment">     * </span><span class="ST0">Uses</span> <span class="ST0">the</span> <span class="ST0">specified</span> <span class="ST0">activation</span> <span class="ST0">function</span><span class="ST0">.</span> <span class="comment">Currently</span> <span class="comment">this</span> <span class="comment">is</span> <span class="comment">reLU</span><span class="comment">.</span>
733 <span class="comment">     * </span><span class="ST0">@param</span> <span class="comment">x</span> <span class="comment">The</span> <span class="comment">input</span> 
734 <span class="comment">     * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">return</span>
735      <span class="comment">*/</span>
736     <span class="literal">private</span> <span class="literal">static</span> <span class="literal">double</span> activation(<span class="literal">double</span> x){
737         <span class="literal">if</span>(x&gt;0.0){
738             <span class="literal">return</span> x;
739         }<span class="literal">else</span>{
740             <span class="literal">return</span> 0.0;
741         } 
742     }
743     <span class="comment">/**</span>
744 <span class="comment">     * </span><span class="ST0">Returns</span> <span class="ST0">the</span> <span class="ST0">gradient</span> <span class="ST0">of</span> <span class="ST0">the</span> <span class="ST0">chosen</span> <span class="ST0">activation</span><span class="ST0">.</span> <span class="comment">Currently</span> <span class="comment">this</span> <span class="comment">returns</span> <span class="comment">the</span> <span class="comment">gradient</span> <span class="comment">for</span> <span class="comment">reLU</span><span class="comment">.</span>
745 <span class="comment">     * </span><span class="ST0">@param</span> <span class="comment">x</span> <span class="comment">The</span> <span class="comment">input</span>
746 <span class="comment">     * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">return</span> <span class="comment">value</span>
747      <span class="comment">*/</span>
748     <span class="literal">private</span> <span class="literal">static</span> <span class="literal">double</span> activationDerivative(<span class="literal">double</span> x){
749        <span class="literal">if</span>(x&gt;0.0){
750             <span class="literal">return</span> 1.0;
751         }<span class="literal">else</span>{
752             <span class="literal">return</span> 0.0;
753         } 
754     }
755     <span class="comment">/**</span>
756 <span class="comment">     * </span><span class="ST0">Gets</span> <span class="ST0">a</span> <span class="ST0">random</span> <span class="ST0">number</span> <span class="ST0">between</span> <span class="ST0">one</span> <span class="ST0">and</span> <span class="ST0">minus</span> <span class="ST0">one</span> <span class="ST0">using</span> <span class="ST0">the</span> <span class="ST0">xavier</span> <span class="ST0">technique</span><span class="ST0">.</span>
757 <span class="comment">     * </span><span class="ST0">@param</span> <span class="comment">numberOfNeuronsInLayerBelow</span> <span class="comment">The</span> <span class="comment">number</span> <span class="comment">of</span> <span class="comment">neurons</span> <span class="comment">in</span> <span class="comment">the</span> <span class="comment">layer</span> <span class="comment">below</span>
758 <span class="comment">     * </span><span class="ST0">@param</span> <span class="comment">numberOfNeuronsInLayer</span> <span class="comment">The</span> <span class="comment">number</span> <span class="comment">of</span> <span class="comment">neurons</span> <span class="comment">in</span> <span class="comment">the</span> <span class="comment">current</span> <span class="comment">layer</span>
759 <span class="comment">     * </span><span class="ST0">@return</span> <span class="comment">The</span> <span class="comment">random</span> <span class="comment">number</span>
760      <span class="comment">*/</span>
761     <span class="literal">private</span> <span class="literal">double</span> getRandDoubleBetweenOneAndMinusOne(<span class="literal">int</span> numberOfNeuronsInLayerBelow,<span class="literal">int</span> numberOfNeuronsInLayer){
762         <span class="literal">return</span> RAND.nextGaussian()*Math.sqrt(2.0/((<span class="literal">double</span>)(numberOfNeuronsInLayer+numberOfNeuronsInLayerBelow)));
763        <span class="comment">// return (rnd.nextGaussian()*Math.sqrt(1.0/(double)numberOfInputsForWeightsInThisLayer)); //xavier init for tanh/sigmoid</span>
764     }
765     <span class="comment">/**</span>
766 <span class="comment">     * </span><span class="comment">Gets</span> <span class="comment">the</span> <span class="comment">best</span> <span class="comment">network</span> <span class="comment">with</span> <span class="comment">this</span> <span class="comment">structure</span> <span class="comment">and</span> <span class="comment">loads</span> <span class="comment">it</span> <span class="comment">to</span> <span class="comment">a</span> <span class="comment">text</span> <span class="comment">file</span>
767      <span class="comment">*/</span>
768     <span class="literal">public</span> <span class="literal">void</span> writeBestNetToBestFile(){
769         <span class="literal">double</span>[]bestNet=<span class="literal">this</span>.loadBestNetWithThisArchitecture();
770         <span class="literal">try</span>{
771             FileWriter w = <span class="literal">new</span> FileWriter(BEST_NET_FILE_NAME,<span class="literal">false</span>);
772             BufferedWriter buffWrite = <span class="literal">new</span> BufferedWriter(w);
773             buffWrite.write(String.valueOf(bestNet[0]));
774             <span class="literal">for</span>(<span class="literal">int</span> i=1;i&lt;bestNet.length;i++){
775                 buffWrite.newLine();
776                 buffWrite.write(String.valueOf(bestNet[i]));
777             }
778             buffWrite.flush();w.flush();
779             buffWrite.close();w.close();
780         }<span class="literal">catch</span>(IOException e){
781             System.out.println(<span class="string">&quot;</span><span class="string">An unexpected error occured </span><span class="string">&quot;</span>+e);
782         }
783     }
784 }
785 
</pre></body>
</html>
